{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5182fd3b-fa90-4a6d-b146-4a5c631368eb",
   "metadata": {},
   "source": [
    "# Formalia\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/TheYuanLiao/comsocsci2025/wiki/Assignments) carefully before proceeding. The page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "__If you fail to follow these simple instructions, it will negatively impact your grade!__\n",
    "\n",
    "**Due date and time**: The assignment is due on Mar 4th at 23:59. Hand in your Jupyter notebook file (with extension `.ipynb`) via DTU Learn _(Assignment 1)_. \n",
    "\n",
    "Remember to include in the first cell of your notebook:\n",
    "* the link to your group's Git repository \n",
    "* group members' contributions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2182d781-3727-4ff5-9b58-689381202a99",
   "metadata": {},
   "source": [
    "## Part 1: Web-scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e8e50-307c-4315-bed5-1de2fd37d743",
   "metadata": {},
   "source": [
    "> **Exercise: Web-scraping the list of participants to the International Conference in Computational Social Science**    \n",
    ">\n",
    "> You can find the programme of the 2023 edition of the conference at [this link](https://ic2s2-2023.org/program). As you can see the conference programme included many different contributions: keynote presentations, parallel talks, tutorials, posters. \n",
    "> 1. Inspect the HTML of the page and use web-scraping to get the names of all researchers that contributed to the conference in 2023. The goal is the following: (i) get as many names as possible including: keynote speakers, chairs, authors of parallel talks and authors of posters; (ii) ensure that the collected names are complete and accuarate as reported in the website (e.g. both first name and family name); (iii) ensure that no name is repeated multiple times with slightly different spelling. \n",
    "> 2. Some instructions for success: \n",
    ">    * First, inspect the page through your web browser to identify the elements of the page that you want to collect. Ensure you understand the hierarchical structure of the page, and where the elements you are interested in are located within this nested structure.   \n",
    ">    * Use the [BeautifulSoup Python package](https://pypi.org/project/beautifulsoup4/) to navigate through the hierarchy and extract the elements you need from the page. \n",
    ">    * You can use the [find_all](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all) method to find elements that match specific filters. Check the [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) of the library for detailed explanations on how to set filters.  \n",
    ">    * Parse the strings to ensure that you retrieve \"clean\" author names (e.g. remove commas, or other unwanted charachters)\n",
    ">    * The overall idea is to adapt the procedure I have used [here](https://nbviewer.org/github/lalessan/comsocsci2023/blob/master/additional_notebooks/ScreenScraping.ipynb) for the specific page you are scraping. \n",
    "> 3. Create the set of unique researchers that joined the conference and *store it into a file*.\n",
    ">     * *Important:* If you notice any issue with the list of names you have collected (e.g. duplicate/incorrect names), come up with a strategy to clean your list as much as possible. \n",
    "> 4. *Optional:* For a more complete represenation of the field, include in your list: (i) the names of researchers from the programme committee of the conference, that can be found at [this link](https://ic2s2-2023.org/program_committee); (ii) the organizers of tutorials, that can be found at [this link](https://ic2s2-2023.org/tutorials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00112ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of researcher names from the program of 2023: 1380\n",
      "\n",
      "First 10 names:\n",
      "Aaron Clauset\n",
      "Aaron J. Schwartz\n",
      "Aaron Schein\n",
      "Aaron Smith\n",
      "Abbas Haidar\n",
      "Abby Smith\n",
      "Abdulkadir Celikkanat\n",
      "Abdullah Almaatouq\n",
      "Abdullah Zameek\n",
      "Adam Finnemann\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = \"https://ic2s2-2023.org/program\"\n",
    "\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "sections = soup.find_all(\"section\", id=\"main\")\n",
    "\n",
    "# Initialize a set to store unique names\n",
    "researcher_names_2023 = set()\n",
    "\n",
    "# Regular expression to match names inside <i>\n",
    "name_pattern = re.compile(r\"\\b[A-Z][a-z]+(?:\\s[A-Z]\\.?)?(?:\\s[A-Z][a-z]+)+\\b\")\n",
    "\n",
    "for section in sections:\n",
    "    for i_tag in section.find_all(\"i\"):\n",
    "        text = i_tag.get_text()\n",
    "        \n",
    "        matches = name_pattern.findall(text)\n",
    "        for match in matches:\n",
    "            researcher_names_2023.add(match)\n",
    "\n",
    "print(f\"Number of researcher names from the program of 2023: {len(researcher_names_2023)}\\n\")\n",
    "print(\"First 10 names:\")\n",
    "print(\"\\n\".join(list(sorted(researcher_names_2023))[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52735a6d",
   "metadata": {},
   "source": [
    "### Storing the unique researcher names in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02ff86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"files/researcher_names_2023.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(sorted(list(researcher_names_2023))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00ae3a",
   "metadata": {},
   "source": [
    "> 5. How many unique researchers do you get?\n",
    "\n",
    "**Answer:**  \n",
    "We found 1380 unique reserchers\n",
    "\n",
    "> 6. Explain the process you followed to web-scrape the page. Which choices did you make to accurately retreive as many names as possible? Which strategies did you use to assess the quality of your final list? Explain your reasoning and your choices __(answer in max 150 words)__.\n",
    "\n",
    "**Answer:**  \n",
    "We started by creating a `BeautifulSoup()` instance to connect to the URL. Each day's program was stored in a `<section>` element with an id of \"main,\" so we used the soup instance to find all these sections. We then made a `set()` called \"researcher_names_2023\" to store unique names. After inspecting the sections, we noticed that names were inside `<i>` tags. We looped through each section, found all `i` elements, and extracted the text. Since one `<i>` tag could contain multiple names, we used a regular expression to match each name and added them to the set. ChatGPT helped create the pattern, which matches names like:  \n",
    "- `\"John Smith\"`  \n",
    "- `\"John A. Smith\"`  \n",
    "- `\"John A Smith\"`  \n",
    "- `\"John Michael Smith\"` or `\"John A. Michael Smith\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac17914c-f125-4f70-b1a5-0c56641a0ea0",
   "metadata": {},
   "source": [
    "## Part 2: Ready Made vs Custom Made Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25059f1b-25c3-47eb-8278-903bd7608576",
   "metadata": {},
   "source": [
    "> **Exercise: Ready made data vs Custom made data** In this exercise, I want to make sure you have understood they key points of my lecture and the reading. \n",
    ">\n",
    "> 1. What are pros and cons of the custom-made data used in Centola's experiment (the first study presented in the lecture) and the ready-made data used in Nicolaides's study (the second study presented in the lecture)? You can support your arguments based on the content of the lecture and the information you read in Chapter 2.3 of the book __(answer in max 150 words)__.\n",
    "\n",
    "**Answer:**  \n",
    "### Centola's experiment\n",
    "#### Pros:\n",
    "- The results are useful in order to best select which people to select the next time  \n",
    "- The data is nonreactive, which means that they are unaware about the experiment and therefore aren't biased  \n",
    "- They have access to all data since it's custom made  \n",
    "- The data is clean, no bots no spam  \n",
    "\n",
    "#### Cons:\n",
    "- Not much data  \n",
    "- Takes time  \n",
    "- The demographic isn't very wide, since the participants have to agree to the survey. This requires a special type of person  \n",
    "\n",
    "### Nicolaides's study\n",
    "#### Pros:\n",
    "- Always on, the data is always updated  \n",
    "- Large data  \n",
    "- Nonreactive  \n",
    "- Complete, clean and accessible data, they have all variables required\n",
    "\n",
    "#### Cons:\n",
    "- The data is nonrepresentative, the population isn't varied, they collect data from the app that only runners use  \n",
    "- There could be many confounders, such as variables as weather and vacations\n",
    "\n",
    "\n",
    "> 2. How do you think these differences can influence the interpretation of the results in each study? __(answer in max 150 words)__\n",
    "\n",
    "**Answer:**  \n",
    "For Centola's experiment the data is controlled and clean, so the observed effects are causally related to the design of the experiment. However since the data is nonpresentative it can't necessarily be generalized to the broader population. The findings may only apply to a specific demographic that self-selected into the study, and the effects may not hold in real-world, more diverse settings.\n",
    "\n",
    "For Nicolaides's study uses large-scale, real-world data, capturing social behavior over time. While this improves generalizability, confounders like weather or socioeconomic factors may bias results. Additionally, since the data comes from a runners' app, findings may not apply to non-runners.\n",
    "\n",
    "#### The trade-off between control vs. generalizability is key:\n",
    "Centola's experiment provides high internal validity (causality is clear) but low external validity (hard to generalize).\n",
    "Nicolaides's study offers high external validity (reflects real-world behavior) but lower internal validity (difficult to establish causal relationships)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654c7606-9542-4ecd-9f6e-1c5e298883d6",
   "metadata": {},
   "source": [
    "## Part 3: Gathering Research Articles using the OpenAlex API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed489a-1f99-4124-af1b-97f332fb0fe1",
   "metadata": {},
   "source": [
    "> **Exercise : Collecting Research Articles from IC2S2 Authors**\n",
    ">\n",
    ">In this exercise, we'll leverage the OpenAlex API to gather information on research articles authored by participants of the IC2S2 2024 (NOT 2023) conference, referred to as *IC2S2 authors*. **Before you start, please ensure you read through the entire exercise.**\n",
    ">\n",
    "> \n",
    "> **Steps:**\n",
    ">  \n",
    "> 1. **Retrieve Data:** Starting with the *authors* you identified in Week 2, Exercise 2, use the OpenAlex API [works endpoint](https://docs.openalex.org/api-entities/works) to fetch the research articles they have authored. For each article, retrieve the following details:\n",
    ">    - _id_: The unique OpenAlex ID for the work.\n",
    ">    - _publication_year_: The year the work was published.\n",
    ">    - _cited_by_count_: The number of times the work has been cited by other works.\n",
    ">    - _author_ids_: The OpenAlex IDs for the authors of the work.\n",
    ">    - _title_: The title of the work.\n",
    ">    - _abstract_inverted_index_: The abstract of the work, formatted as an inverted index.\n",
    "> \n",
    ">     **Important Note on Paging:** By default, the OpenAlex API limits responses to 25 works per request. For more efficient data retrieval, I suggest to adjust this limit to 200 works per request. Even with this adjustment, you will need to implement pagination to access all available works for a given query. This ensures you can systematically retrieve the complete set of works beyond the initial 200. Find guidance on implementing pagination [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/paging#cursor-paging).\n",
    ">\n",
    "> 2. **Data Storage:** Organize the retrieved information into two Pandas DataFrames and save them to two files in a suitable format:\n",
    ">    - The *IC2S2 papers* dataset should include: *id, publication\\_year, cited\\_by\\_count, author\\_ids*.\n",
    ">    - The *IC2S2 abstracts* dataset should include: *id, title, abstract\\_inverted\\_index*.\n",
    ">  \n",
    ">\n",
    "> **Filters:**\n",
    "> To ensure the data we collect is relevant and manageable, apply the following filters:\n",
    "> \n",
    ">    - Only include *IC2S2 authors* with a total work count between 5 and 5,000.\n",
    ">    - Retrieve only works that have received more than 10 citations.\n",
    ">    - Limit to works authored by fewer than 10 individuals.\n",
    ">    - Include only works relevant to Computational Social Science (focusing on: Sociology OR Psychology OR Economics OR Political Science) AND intersecting with a quantitative discipline (Mathematics OR Physics OR Computer Science), as defined by their [Concepts](https://docs.openalex.org/api-entities/works/work-object#concepts). *Note*: here we only consider Concepts at *level=0* (the most coarse definition of concepts). \n",
    ">\n",
    "> **Efficiency Tips:**\n",
    "> Writing efficient code in this exercise is **crucial**. To speed up your process:\n",
    "> - **Apply filters directly in your request:** When possible, use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) of the *works* endpoint to apply the filters above directly in your API request, ensuring only relevant data is returned. Learn about combining multiple filters [here](https://docs.openalex.org/how-to-use-the-api/get-lists-of-entities/filter-entity-lists).  \n",
    "> - **Bulk requests:** Instead of sending one request for each author, you can use the [filter parameter](https://docs.openalex.org/api-entities/works/filter-works) to query works by multiple authors in a single request. *Note: My testing suggests that can only include up to 25 authors per request.*\n",
    "> - **Use multiprocessing:** Implement multiprocessing to handle multiple requests simultaneously. I highly recommmend [Joblib’s Parallel](https://joblib.readthedocs.io/en/stable/) function for that, and [tqdm](https://tqdm.github.io/) can help monitor progress of your jobs. Remember to stay within [the rate limit](https://docs.openalex.org/how-to-use-the-api/rate-limits-and-authentication) of 10 requests per second.\n",
    ">\n",
    ">\n",
    ">   \n",
    "> For reference, employing these strategies allowed me to fetch the data in about 30 seconds using 5 cores on my laptop. I obtained a dataset of approximately 25 MB (including both the *IC2S2 abstracts* and *IC2S2 papers* files)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d1364d",
   "metadata": {},
   "source": [
    "### Functions for retrieving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80175db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def batch_list(lst, batch_size):\n",
    "    \"\"\"Yield successive chunks of size batch_size from lst.\"\"\"\n",
    "    for i in range(0, len(lst), batch_size):\n",
    "        yield lst[i:i+batch_size]\n",
    "\n",
    "def fetch_works_for_batch(author_ids, per_page=200, sleep_time=0.2):\n",
    "    \"\"\"\n",
    "    Fetch works from OpenAlex for a batch of author IDs.\n",
    "    \n",
    "    Parameters:\n",
    "      - author_ids: list of author OpenAlex IDs (e.g. \"https://openalex.org/A123456789\")\n",
    "      - filter_params: additional filter string (e.g. \"cited_by_count:>10,authorship_count:<10\")\n",
    "      - per_page: number of works per page (max 200)\n",
    "      - sleep_time: pause between pages (to respect rate limits)\n",
    "      \n",
    "    Returns:\n",
    "      - List of work dictionaries.\n",
    "    \"\"\"\n",
    "    works_endpoint = \"https://api.openalex.org/works\"\n",
    "    all_works = []\n",
    "    # Build filter for author IDs (using the OR operator \"|\")\n",
    "    filter_query = \"authorships.author.id:\" + \"|\".join(author_ids)\n",
    "    filter_query += \",cited_by_count:>10,authors_count:<10\"\n",
    "    filter_query += \",concepts.id:\" + \"|\".join([\"https://openalex.org/C144024400\", \"https://openalex.org/C15744967\", \"https://openalex.org/C162324750\", \"https://openalex.org/C17744445\", \"https://openalex.org/C33923547\", \"https://openalex.org/C121332964\", \"https://openalex.org/C41008148\"])\n",
    "\n",
    "    cursor = \"*\"\n",
    "    while True:\n",
    "        params = {\n",
    "            \"filter\": filter_query,\n",
    "            \"per_page\": per_page,\n",
    "            \"cursor\": cursor,\n",
    "            \"select\": \"id,publication_year,cited_by_count,authorships,abstract_inverted_index,title,concepts\"\n",
    "        }\n",
    "        response = requests.get(works_endpoint, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching works for authors {author_ids}: {response.status_code}\")\n",
    "            break\n",
    "        data = response.json()\n",
    "        results = data.get(\"results\", [])\n",
    "        all_works.extend(results)\n",
    "        meta = data.get(\"meta\", {})\n",
    "        next_cursor = meta.get(\"next_cursor\")\n",
    "        if not next_cursor:\n",
    "            break\n",
    "        cursor = next_cursor\n",
    "        time.sleep(sleep_time)\n",
    "    return all_works\n",
    "\n",
    "def fetch_works_for_authors(author_ids, batch_size=25, n_jobs=5):\n",
    "    \"\"\"\n",
    "    Given a list of author IDs, split them into batches and fetch works in parallel.\n",
    "    \n",
    "    Returns:\n",
    "      - List of work dictionaries.\n",
    "    \"\"\"\n",
    "    batches = list(batch_list(author_ids, batch_size))\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(fetch_works_for_batch)(batch) for batch in tqdm(batches, desc=\"Fetching works for authors\")\n",
    "    )\n",
    "    # Flatten list of lists\n",
    "    all_works = [work for sublist in results for work in sublist]\n",
    "    return all_works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36003085",
   "metadata": {},
   "source": [
    "### Retrieving works data for each researcher name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32c974",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_df = pd.read_csv('files/researchers_data_2024.csv')\n",
    "filtered_authors = authors_df[(authors_df['Works Count'] >= 5) & (authors_df['Works Count'] <= 5000)]\n",
    "filtered_authors_ids = filtered_authors[\"ID\"].tolist()\n",
    "\n",
    "all_works = fetch_works_for_authors(filtered_authors_ids)\n",
    "print(f\"Total works fetched for IC2S2 2024 authors: {len(all_works)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabd1ac0",
   "metadata": {},
   "source": [
    "In the request we filtered for the number of citations, the number of authors, and for the concepts. Now we need to only keep the works relevant to Computational Social Science and that intersects with a quantitative discipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a1d100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_works_by_concepts(works, css_concepts, quantitative_concepts):\n",
    "    \"\"\"\n",
    "    Filter works to include only those that have at least one level-0 concept \n",
    "    from each of the following groups:\n",
    "      - Computational Social Science: e.g. Sociology, Psychology, Economics, Political Science\n",
    "      - Quantitative disciplines: e.g. Mathematics, Physics, Computer Science\n",
    "    \"\"\"\n",
    "    filtered = []\n",
    "    # Pre-lowercase the concept names for easier comparison\n",
    "    css_concepts_lower = [c.lower() for c in css_concepts]\n",
    "    quantitative_concepts_lower = [q.lower() for q in quantitative_concepts]\n",
    "    \n",
    "    for work in works:\n",
    "        concepts = work.get(\"concepts\", [])\n",
    "        css_found = False\n",
    "        quantitative_found = False\n",
    "        for concept in concepts:\n",
    "            if concept.get(\"level\") == 0:\n",
    "                name = concept.get(\"display_name\", \"\").lower()\n",
    "                if name in css_concepts_lower:\n",
    "                    css_found = True\n",
    "                if name in quantitative_concepts_lower:\n",
    "                    quantitative_found = True\n",
    "        if css_found and quantitative_found:\n",
    "            filtered.append(work)\n",
    "    return filtered\n",
    "\n",
    "css_concepts = [\"Sociology\", \"Psychology\", \"Economics\", \"Political Science\"]\n",
    "quantitative_concepts = [\"Mathematics\", \"Physics\", \"Computer Science\"]\n",
    "filtered_works = filter_works_by_concepts(all_works, css_concepts, quantitative_concepts)\n",
    "print(f\"Total works after concept filtering: {len(filtered_works)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a7a38",
   "metadata": {},
   "source": [
    "### Storing the data in csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e70b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_papers = []\n",
    "authors_abstracts = []\n",
    "\n",
    "for work in filtered_works:\n",
    "    # Extract author IDs from the \"authorships\" field\n",
    "    authors = [auth.get(\"author\", {}).get(\"id\") for auth in work.get(\"authorships\", []) if auth.get(\"author\", {}).get(\"id\")]\n",
    "    authors_papers.append({\n",
    "        \"id\": work.get(\"id\"),\n",
    "        \"publication_year\": work.get(\"publication_year\"),\n",
    "        \"cited_by_count\": work.get(\"cited_by_count\"),\n",
    "        \"author_ids\": authors\n",
    "    })\n",
    "    authors_abstracts.append({\n",
    "        \"id\": work.get(\"id\"),\n",
    "        \"title\": work.get(\"title\"),\n",
    "        \"abstract_inverted_index\": work.get(\"abstract_inverted_index\")\n",
    "    })\n",
    "\n",
    "authors_papers_df = pd.DataFrame(authors_papers)\n",
    "authors_abstracts_df = pd.DataFrame(authors_abstracts)\n",
    "\n",
    "authors_papers_df.to_csv(\"authors_papers.csv\", index=False)\n",
    "authors_abstracts_df.to_csv(\"authors_abstracts.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221b0d6",
   "metadata": {},
   "source": [
    "### Retrieving the ids of unique co-author researchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3720811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "#authors_papers_df = pd.read_csv(\"authors_papers.csv\")\n",
    "authors_papers = authors_papers_df.to_dict(orient='records')\n",
    "for paper in authors_papers:\n",
    "    if isinstance(paper['author_ids'], str):\n",
    "        paper['author_ids'] = ast.literal_eval(paper['author_ids'])\n",
    "\n",
    "all_author_ids_in_works = set()\n",
    "for paper in authors_papers:\n",
    "    for aid in paper[\"author_ids\"]:\n",
    "        all_author_ids_in_works.add(aid)\n",
    "\n",
    "# Identify co-author IDs by removing the IC2S2 authors\n",
    "authors_ids_set = set(filtered_authors_ids)\n",
    "coauthor_ids = list(all_author_ids_in_works - authors_ids_set)\n",
    "print(f\"Total unique co-author IDs: {len(coauthor_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e4349e",
   "metadata": {},
   "source": [
    "> **Data Overview and Reflection questions:** Answer the following questions: \n",
    "> - **Dataset summary.** How many works are listed in your *IC2S2 papers* dataframe? How many unique researchers have co-authored these works? \n",
    "\n",
    "**Answer:**  \n",
    "We obtained 12746 papers that we stored into the dataframe. We extracted out all the co-authors and found there to be 16912 unique co-authors.\n",
    "\n",
    "> - **Efficiency in code.** Describe the strategies you implemented to make your code more efficient. How did your approach affect your code's execution time? __(answer in max 150 words)__\n",
    "\n",
    "**Answer:**  \n",
    "To improve efficiency, we implemented batch processing and parallelization. The `batch_list` function splits author IDs into smaller batches, optimizing the API request size and preventing timeouts. By using `joblib`'s `Parallel` and `delayed` with `n_jobs=5`, we parallelized API requests, significantly reducing execution time. we used a cursor-based pagination approach in `fetch_works_for_batch`, allowing uninterrupted data retrieval until all results were fetched. Adding a brief `sleep_time` of 0.2 seconds between requests maintained API rate limits without unnecessary delays. Before requesting the API we only kept the ids of authors with a total work count between 5 and 5,000. Then we applied filters directly to the request which greatly reduced the number of works to retrieve.\n",
    "\n",
    "> - **Filtering Criteria and Dataset Relevance** Reflect on the rationale behind setting specific thresholds for the total number of works by an author, the citation count, the number of authors per work, and the relevance of works to specific fields. How do these filtering criteria contribute to the relevance of the dataset you compiled? Do you believe any aspects of Computational Social Science research might be underrepresented or overrepresented as a result of these choices? __(answer in max 150 words)__\n",
    "\n",
    "**Answer:**  \n",
    "The filtering criteria helped make the dataset more relevant and manageable. By setting a range of 5 to 5000 works per author, we avoided including authors with too few or too many works, focusing on active researchers. The citation filter ensured we included impactful research, and limiting the number of authors per work helped highlight studies with clear individual contributions. We also filtered works to specific fields related to Computational Social Science to keep the dataset focused.\n",
    "\n",
    "These filtering choices might mean we missed new researchers with fewer citations. Filtering by specific fields could also mean we missed other interdisciplinary studies relevant to Computational Social Science outside of the specific fields we chose. This could lead to overrepresenting well-established areas with lots of citations. So while the filtering made our dataset more relevant, it might create a bias toward established researchers and topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f72dca3-246a-4056-b99c-2f14ccef7fef",
   "metadata": {},
   "source": [
    "## Part 4: The Network of Computational Social Scientists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147f812-b857-4b46-a762-fcb8661cb47c",
   "metadata": {},
   "source": [
    "> **Exercise: Constructing the Computational Social Scientists Network**\n",
    ">\n",
    "> In this exercise, we will create a network of researchers in the field of Computational Social Science using the NetworkX library. In our network, nodes represent authors of academic papers, with a direct link from node _A_ to node _B_ indicating a joint paper written by both. The link's weight reflects the number of papers written by both _A_ and _B_.\n",
    ">\n",
    "> **Part 1: Network Construction**\n",
    ">\n",
    "> 1. **Weighted Edgelist Creation:** Start with your dataframe of *papers*. Construct a _weighted edgelist_ where each list element is a tuple containing three elements: the _author ids_ of two collaborating authors and the total number of papers they've co-authored. Ensure each author pair is listed only once. \n",
    ">\n",
    "> 2. **Graph Construction:**\n",
    ">    - Use NetworkX to create an undirected [``Graph``](https://networkx.org/documentation/stable/reference/classes/graph.html).\n",
    ">    - Employ the [`add_weighted_edges_from`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_weighted_edges_from.html#networkx.Graph.add_weighted_edges_from) function to populate the graph with the weighted edgelist from step 1, creating a weighted, undirected graph.\n",
    ">\n",
    "> 3. **Node Attributes:**\n",
    ">    - For each node, add attributes for the author's _display name_, _country_, _citation count_, and the _year of their first publication_ in Computational Social Science. The _display name_ and _country_ can be retrieved from your _authors_ dataset. The _year of their first publication_ and the _citation count_  can be retrieved from the _papers_ dataset.\n",
    ">    - Save the network as a JSON file.\n",
    ">      \n",
    "> **Part 2: Preliminary Network Analysis**\n",
    "> Now, with the network constructed, perform a basic analysis to explore its features.\n",
    "> 1. **Network Metrics:**\n",
    ">    - What is the total number of nodes (authors) and links (collaborations) in the network? \n",
    ">    - Calculate the network's density (the ratio of actual links to the maximum possible number of links). Would you say that the network is sparse? Justify your answer.\n",
    ">    - Is the network fully connected (i.e., is there a direct or indirect path between every pair of nodes within the network), or is it disconnected?\n",
    ">    - If the network is disconnected, how many connected components does it have? A connected component is defined as a subset of nodes within the network where a path exists between any pair of nodes in that subset. \n",
    ">    - How many isolated nodes are there in your network?  An isolated node is defined as a node with no connections to any other node in the network.\n",
    ">    - Discuss the results above on network density, and connectivity. Are your findings in line with what you expected? Why?  __(answer in max 150 words)__\n",
    "> \n",
    "> 3. **Degree Analysis:**\n",
    ">    - Compute the average, median, mode, minimum, and maximum degree of the nodes. Perform the same analysis for node strength (weighted degree). What do these metrics tell us about the network? __(answer in max 150 words)__\n",
    "> \n",
    "> 4. **Top Authors:**\n",
    ">    - Identify the top 5 authors by degree. What role do these node play in the network? \n",
    ">    - Research these authors online. What areas do they specialize in? Do you think that their work aligns with the themes of Computational Social Science? If not, what could be possible reasons? __(answer in max 150 words)__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35130cc2-dbf6-42db-8901-2475ede6444d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\josef\\AppData\\Local\\Temp\\ipykernel_28160\\2948957795.py:6: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  papers_df = pd.read_csv(\"C:\\DTU\\Fjerde-semester\\Social_informatik\\comsocsci2025\\lectures\\papers_combined.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            author1                           author2  weight\n",
      "0  https://openalex.org/A5014647140  https://openalex.org/A5082953212       2\n",
      "1  https://openalex.org/A5014647140  https://openalex.org/A5067142016       4\n",
      "2  https://openalex.org/A5067142016  https://openalex.org/A5082953212       1\n",
      "3  https://openalex.org/A5008033989  https://openalex.org/A5014647140       5\n",
      "4  https://openalex.org/A5008033989  https://openalex.org/A5067142016       4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "\n",
    "# Indlæs data\n",
    "papers_df = pd.read_csv(\"C:\\DTU\\Fjerde-semester\\Social_informatik\\comsocsci2025\\lectures\\papers_combined.csv\")\n",
    "\n",
    "# Omdan 'author_ids' fra streng til liste\n",
    "papers_df[\"author_ids\"] = papers_df[\"author_ids\"].apply(eval)  # Evaluerer strengen som en liste\n",
    "\n",
    "# Dictionary til at tælle forfattersamarbejder\n",
    "coauthor_counts = defaultdict(int)\n",
    "\n",
    "# Gå igennem hver artikel og find samarbejdende forfattere\n",
    "for author_list in papers_df[\"author_ids\"]:\n",
    "    for author1, author2 in combinations(author_list, 2):\n",
    "        pair = tuple(sorted((author1, author2)))  # Sortér for at undgå duplikater\n",
    "        coauthor_counts[pair] += 1\n",
    "\n",
    "# Omdan til en vægtet edgelist (liste af tuples)\n",
    "weighted_edgelist = [(a, b, count) for (a, b), count in coauthor_counts.items()]\n",
    "\n",
    "# Konverter til en DataFrame for bedre visning og lagring\n",
    "edgelist_df = pd.DataFrame(weighted_edgelist, columns=[\"author1\", \"author2\", \"weight\"])\n",
    "\n",
    "# Gem resultatet til en CSV-fil\n",
    "edgelist_df.to_csv(\"weighted_edgelist.csv\", index=False)\n",
    "\n",
    "# Udskriv de første rækker\n",
    "print(edgelist_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d28ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
